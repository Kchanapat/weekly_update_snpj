# UPDATE WEEK 2
**Work Done**
- file knowledge base: แก้ให้สามารถ label ให้มีหลาย actegory ได้
- file question: label เพิ่มว่าอันไหนใช้ ai generated หรือ คิดเอง
- Benchmark สำหรับประเมิน RAG ในการดึงข้อมูลจาก knowledge base ให้ตรงกับคำถาม
  - โดยกำหนดให้คำถามแต่ละข้อมีป้ายกำกับหมวดหมู่ที่ชัดเจนในสองระดับ ได้แก่:
    - Category 1: หมวดหมู่หลัก ได้แก่ หัตถการ / อสาการภาวะแทรกซ้อน / การปฏิบัติตัวหลังการทำหัตถการ
    - Category 2: หมวดย่อย เช่น pain, swelling, local anesthesia, oral hygiene ฯลฯ ซึ่งเป็น subset ของ Category 1
  - จากนั้นได้ดำเนินการดังนี้:
    - สร้างชุดคำถามที่มีการ label แล้ว ว่าคำถามแต่ละข้อเกี่ยวข้องกับ Category1 และ Category2 ใด,
    - สร้าง knowledge base ที่จัดเก็บเป็น chunks โดยแต่ละ chunk มี metadata ระบุ category1 และ category2 ชัดเจน,
    - ทดสอบระบบ RAG โดยส่งคำถามเข้าสู่ระบบ และบันทึก chunks ที่ถูกดึงออกมา,
    - ประเมินผล โดยตรวจสอบว่า chunk ที่ระบบดึงมาในอันดับต้น ๆ (Top‑k) มีหมวดหมู่ตรงกับที่ label ไว้หรือไม่,
  - ใช้เกณฑ์ เช่น Top‑K Accuracy, Precision / Recall / F1-score ต่อ category → เพื่อดูว่าแต่ละหมวด RAG ดึงได้แม่นแค่ไหน และดึงครบหรือเปล่า
  - <img width="1281" height="949" alt="image" src="https://github.com/user-attachments/assets/73479504-17f4-4906-a739-bb7dfa439617" />
  - ในการประเมิน RAG ว่าดึงตรงกับ category ที่ควรจะเป็นมั้ย เราต้องวัดว่า
    - Precision (ความแม่น) → ที่ RAG ดึงมาว่าเป็นหมวดนี้ มันถูกจริงกี่อัน
    - Recall (ความครอบคลุม) → ในหมวดนั้น ๆ จริง ๆ ควรดึงกี่อัน แล้วดึงถูกกี่อัน
    - F1-score → ค่าเฉลี่ยของความแม่น (precision) กับความครอบคลุม (recall) ให้เป็นค่ากลาง ๆ
    - category-level = เราไม่ได้วัดแค่ “คำถามนี้ตอบถูกมั้ย” แบบทีละข้อเท่านั้นแต่เราจะวัดว่า category นี้ RAG ดึงได้แม่นมั้ย ครบมั้ย โดยรวม
    - ✳ex: สมมติเรามี 10 คำถามที่คำตอบควรอยู่ในหมวด “food”
      - สมมติ RAG ดึงมาแบบนี้:
      - 8 คำถามที่มันดึงมา แล้วบอกว่าอยู่ใน “food”
      - จาก 8 อันนั้น มี 6 อันที่ถูกจริง (คือควรอยู่ใน food)
      - นอกนั้นอีก 2 อัน จริง ๆ ควรอยู่หมวดอื่น (เช่น oral-hygiene)
      - คำนวณได้:
      - Precision ของ food = 6/8 = 0.75 (คือบอกว่าอยู่หมวดนี้ แต่จริง ๆ ถูกแค่ 6 จาก 8)
      - Recall ของ food = 6/10 = 0.6 (คือจริง ๆ มี 10 อันควรอยู่ใน food แต่มันหาเจอแค่ 6)
      - F1-score = 2 × (0.75 × 0.6) / (0.75 + 0.6) ≈ 0.666

